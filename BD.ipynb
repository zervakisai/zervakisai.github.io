{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqrmayGkoKx-"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿ full dataset ÏƒÎµ streaming mode\n",
        "dataset = load_dataset(\"bookcorpus\", split=\"train\", streaming=True)"
      ],
      "metadata": {
        "id": "qEfhl6XdoOKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"bookcorpus.csv\"\n",
        "\n",
        "# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î±ÏÏ‡ÎµÎ¯Î¿Ï… Î¼Îµ header\n",
        "with open(csv_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"text\\n\")\n",
        "\n",
        "# Batch export\n",
        "batch_size = 200_000\n",
        "batch = []\n",
        "total = 0\n",
        "\n",
        "for i, example in enumerate(dataset):\n",
        "    batch.append(example)\n",
        "    if (i + 1) % batch_size == 0:\n",
        "        df = pd.DataFrame(batch)\n",
        "        df.to_csv(csv_path, mode='a', header=False, index=False)\n",
        "        total += len(batch)\n",
        "        print(f\"âœ… Saved {total:,} rows...\")\n",
        "        batch = []\n",
        "\n",
        "# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿Ï… batch\n",
        "if batch:\n",
        "    df = pd.DataFrame(batch)\n",
        "    df.to_csv(csv_path, mode='a', header=False, index=False)\n",
        "    total += len(batch)\n",
        "    print(f\"âœ… Final batch saved (total: {total:,})\")\n"
      ],
      "metadata": {
        "id": "DzqD22tDoQgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk -y\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar -xvzf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BookCorpusAnalysis\") \\\n",
        "    .config(\"spark.ui.port\", \"4040\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "M9aC0eBboTcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, split, size\n",
        "\n",
        "# Î£Ï„Î®Î»Î· Î¼Îµ Î±ÏÎ¹Î¸Î¼ÏŒ Î»Î­Î¾ÎµÏ‰Î½\n",
        "df_words = df.withColumn(\"word_count\", size(split(col(\"text\"), \"\\\\s+\")))\n",
        "\n",
        "# Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ word counts\n",
        "df_words.select(\"word_count\").summary(\"count\", \"mean\", \"min\", \"25%\", \"50%\", \"75%\", \"max\").show()\n",
        "\n"
      ],
      "metadata": {
        "id": "8tQohsheoXM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ngrok Î³Î¹Î± Spark UI\n",
        "!wget -q -nc https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-stable-linux-amd64.zip\n",
        "!unzip -n ngrok-stable-linux-amd64.zip\n",
        "# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï… Ngrok token\n",
        "!./ngrok authtoken 2uzOrSfKyZmx3KsoUMO8UpSYJ5x_3yfwivzhuBKEVTJJjJncJ\n",
        "# ÎÎµÎºÎ¹Î½Î¬Î¼Îµ Ï„Î¿ ngrok tunnel Ï€ÏÎ¿Ï‚ Ï„Î·Î½ Ï€ÏŒÏÏ„Î± Ï„Î¿Ï… Spark UI\n",
        "get_ipython().system_raw('./ngrok http 4040 --log=stdout > ngrok.log &')"
      ],
      "metadata": {
        "id": "EWEsEMulohQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time, requests\n",
        "time.sleep(3)\n",
        "\n",
        "try:\n",
        "    r = requests.get('http://localhost:4040/api/tunnels')\n",
        "    ui_url = r.json()['tunnels'][0]['public_url']\n",
        "    print(f\"ğŸš€ Spark UI is live at: {ui_url}\")\n",
        "except:\n",
        "    print(\"âŒ Ngrok Î´ÎµÎ½ ÏƒÏ…Î½Î´Î­Î¸Î·ÎºÎµ\")\n",
        "    !tail -n 20 ngrok.log\n",
        "\n"
      ],
      "metadata": {
        "id": "YJgbAFdRojIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, lower, regexp_replace, split, col, length\n",
        "\n",
        "# ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… + Tokenization + Explode\n",
        "df_tokens = df.select(\"text\").na.drop().repartition(8).select(\n",
        "    explode(\n",
        "        split(\n",
        "            lower(\n",
        "                regexp_replace(col(\"text\"), r\"[^a-zA-Z]\", \" \")\n",
        "            ),\n",
        "            r\"\\s+\"\n",
        "        )\n",
        "    ).alias(\"word\")\n",
        ").filter(length(col(\"word\")) > 2)  # Î±Î³Î½ÏŒÎ·ÏƒÎ· Î¼Î¹ÎºÏÏÎ½ Î»Î­Î¾ÎµÏ‰Î½"
      ],
      "metadata": {
        "id": "jV9AG_beooxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÎŸÎ¼Î±Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎºÎ±Î¹ Î¼Î­Ï„ÏÎ·ÏƒÎ· ÏƒÏ…Ï‡Î½Î¿Ï„Î®Ï„Ï‰Î½\n",
        "word_freq = df_tokens.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
        "\n",
        "# Î ÏÎ¿Î²Î¿Î»Î® top-20 (Spark UI Î´ÎµÎ¯Ï‡Î½ÎµÎ¹ Ï„Î¿ stage)\n",
        "word_freq.show(20, truncate=False)"
      ],
      "metadata": {
        "id": "HhSMdnQQorMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Î¥Ï€Î¿Î¸Î­Ï„Î¿Ï…Î¼Îµ ÏŒÏ„Î¹ Ï„Î¿ dataset Î­ÏÏ‡ÎµÏ„Î±Î¹ Ï‰Ï‚ iterator (stream)\n",
        "def reservoir_sample(stream, k=100000):\n",
        "    reservoir = []\n",
        "    for i, item in enumerate(stream):\n",
        "        if i < k:\n",
        "            reservoir.append(item)\n",
        "        else:\n",
        "            j = random.randint(0, i)\n",
        "            if j < k:\n",
        "                reservoir[j] = item\n",
        "    return reservoir\n",
        "\n",
        "# Î•Ï†Î±ÏÎ¼Î¿Î³Î® ÏƒÎµ bookcorpus.csv\n",
        "with open(\"bookcorpus.csv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    next(f)  # skip header\n",
        "    stream = ({\"text\": line.strip()} for line in f)\n",
        "    sampled = reservoir_sample(stream, k=100000)\n",
        "\n",
        "# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î´ÎµÎ¯Î³Î¼Î±Ï„Î¿Ï‚\n",
        "df_reservoir = pd.DataFrame(sampled)\n",
        "df_reservoir.to_csv(\"reservoir_sample.csv\", index=False)"
      ],
      "metadata": {
        "id": "h6_2yEwroxb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reservoir = spark.read.csv(\"reservoir_sample.csv\", header=True)\n"
      ],
      "metadata": {
        "id": "D-hjgmO2ozkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, lower, regexp_replace, split, col, length\n",
        "\n",
        "df_tokens_rsv = df_reservoir.select(\n",
        "    explode(\n",
        "        split(\n",
        "            lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z]\", \" \")),\n",
        "            \"\\\\s+\"\n",
        "        )\n",
        "    ).alias(\"word\")\n",
        ").filter(length(col(\"word\")) > 2)\n",
        "\n",
        "word_freq_rsv = df_tokens_rsv.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
        "\n",
        "# Î Î¬ÏÎµ Ï„Î± top-20 ÏƒÎµ pandas\n",
        "top_rsv = word_freq_rsv.limit(20).toPandas()\n",
        "top_rsv"
      ],
      "metadata": {
        "id": "-Y8HJ8F4o1Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mmh3\n"
      ],
      "metadata": {
        "id": "xrbE1KfNo3GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mmh3\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class CountMinSketch:\n",
        "    def __init__(self, width=1000, depth=5, seed=42):\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "        self.table = np.zeros((depth, width), dtype=int)\n",
        "        self.seeds = [seed + i for i in range(depth)]\n",
        "\n",
        "    def add(self, key):\n",
        "        for i, s in enumerate(self.seeds):\n",
        "            h = mmh3.hash(key, s) % self.width\n",
        "            self.table[i][h] += 1\n",
        "\n",
        "    def estimate(self, key):\n",
        "        return min(\n",
        "            self.table[i][mmh3.hash(key, s) % self.width]\n",
        "            for i, s in enumerate(self.seeds)\n",
        "        )"
      ],
      "metadata": {
        "id": "Nq4eTDd6o41h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Î›Î¯ÏƒÏ„Î± Î»Î­Î¾ÎµÏ‰Î½ Î±Ï€ÏŒ Ï„Î¿ sample\n",
        "from collections import Counter\n",
        "words = df_reservoir.toPandas()[\"text\"].str.lower().str.replace(r\"[^a-z ]\", \"\", regex=True).str.split()\n",
        "flat_words = [w for sublist in words for w in sublist if len(w) > 2]\n",
        "\n",
        "true_counts = Counter(flat_words)\n",
        "\n",
        "# Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® Î»Î­Î¾ÎµÏ‰Î½ ÏƒÏ„Î¿ Sketch\n",
        "cms = CountMinSketch(width=1000, depth=5)\n",
        "for word in flat_words:\n",
        "    cms.add(word)\n",
        "\n",
        "# Î£Ï…Î³ÎºÏÎ¹Ï„Î¹ÎºÏŒÏ‚ Ï€Î¯Î½Î±ÎºÎ±Ï‚\n",
        "results = []\n",
        "for word, true_val in true_counts.most_common(20):\n",
        "    est_val = cms.estimate(word)\n",
        "    rel_err = 100 * abs(est_val - true_val) / true_val\n",
        "    results.append((word, true_val, est_val, rel_err))\n",
        "\n",
        "import pandas as pd\n",
        "df_sketch = pd.DataFrame(results, columns=[\"word\", \"true\", \"estimated\", \"error_%\"])\n",
        "df_sketch"
      ],
      "metadata": {
        "id": "cd5KPRuno6hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import numpy as np\n",
        "\n",
        "class FMSketch:\n",
        "    def __init__(self, num_hashes=64):\n",
        "        self.num_hashes = num_hashes\n",
        "        self.max_zeroes = np.zeros(num_hashes, dtype=int)\n",
        "\n",
        "    def _hash(self, x, seed):\n",
        "        x = f\"{x}_{seed}\".encode('utf-8')\n",
        "        h = hashlib.sha1(x).hexdigest()\n",
        "        b = bin(int(h, 16))[2:].zfill(160)  # 160-bit string\n",
        "        return b\n",
        "\n",
        "    def add(self, x):\n",
        "        for i in range(self.num_hashes):\n",
        "            b = self._hash(x, i)\n",
        "            self.max_zeroes[i] = max(self.max_zeroes[i], self._rho(b))\n",
        "\n",
        "    def _rho(self, b):\n",
        "        return b.find('1') + 1  # position of first 1\n",
        "\n",
        "    def estimate(self):\n",
        "        return 2 ** (np.median(self.max_zeroes)) / 0.77351"
      ],
      "metadata": {
        "id": "n6dOPi8So9Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flat list of all words Î±Ï€ÏŒ Ï„Î¿ reservoir\n",
        "words = df_reservoir.toPandas()[\"text\"].str.lower().str.replace(r\"[^a-z ]\", \"\", regex=True).str.split()\n",
        "flat_words = [w for sublist in words for w in sublist if len(w) > 2]\n",
        "\n",
        "# Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ ground truth\n",
        "true_unique = len(set(flat_words))\n",
        "\n",
        "# Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Î¼Îµ FM\n",
        "fm = FMSketch(num_hashes=64)\n",
        "for w in flat_words:\n",
        "    fm.add(w)\n",
        "\n",
        "estimated_unique = int(fm.estimate())\n",
        "error_percent = abs(estimated_unique - true_unique) / true_unique * 100\n",
        "\n",
        "print(f\"âœ… True unique words: {true_unique}\")\n",
        "print(f\"âœ… Estimated unique words (FM): {estimated_unique}\")\n",
        "print(f\"ğŸ§® Relative Error: {error_percent:.2f}%\")"
      ],
      "metadata": {
        "id": "4n9FILOso-83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt # Fixed import\n",
        "word_lengths = df_reservoir.toPandas()[\"text\"].str.split().map(len)\n",
        "word_lengths.hist(bins=30, figsize=(8, 4))\n",
        "plt.title(\"Histogram of Word Counts per Record\") # Now plt is pyplot\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.savefig(\"figures/histogram_wordcount.png\")"
      ],
      "metadata": {
        "id": "SOA6cPFApBMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets # Install using pip\n",
        "import pywt # Then import\n",
        "import numpy as np\n",
        "\n",
        "counts = np.array(word_lengths)\n",
        "coeffs = pywt.wavedec(counts, 'db1', level=3)\n",
        "approx, *details = coeffs\n",
        "!pip install PyWavelets # Install using pip\n",
        "import pywt # Then import\n",
        "import numpy as np\n",
        "\n",
        "counts = np.array(word_lengths)\n",
        "coeffs = pywt.wavedec(counts, 'db1', level=3)\n",
        "approx, *details = coeffs\n",
        "reconstructed = pywt.waverec(coeffs, 'db1')\n",
        "\n",
        "compression_ratio = len(approx) / len(counts)\n",
        "ratio = len(approx) / len(counts)"
      ],
      "metadata": {
        "id": "_Ny0e5xspC56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pywt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Î›Î¯ÏƒÏ„Î± Î¼Îµ word counts Î±Î½Î¬ record\n",
        "word_lengths = df_reservoir.toPandas()[\"text\"].str.split().map(len).to_numpy()\n",
        "\n",
        "# Wavelet decomposition (3 levels)\n",
        "coeffs = pywt.wavedec(word_lengths, wavelet='db1', level=3)\n",
        "approx, *details = coeffs\n",
        "\n",
        "# Reconstruction\n",
        "reconstructed = pywt.waverec(coeffs, wavelet='db1')\n",
        "\n",
        "# Resize to original length\n",
        "reconstructed = reconstructed[:len(word_lengths)]\n",
        "\n",
        "# Compression ratio\n",
        "compression_ratio = len(approx) / len(word_lengths)\n",
        "\n",
        "# Plot original vs reconstructed\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(word_lengths[:500], label=\"Original\")\n",
        "plt.plot(reconstructed[:500], label=\"Reconstructed\", linestyle='--')\n",
        "plt.legend()\n",
        "plt.title(\"Original vs Wavelet-Reconstructed Word Counts\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/wavelet_reconstruction.png\")"
      ],
      "metadata": {
        "id": "kXkDu3mopEfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}